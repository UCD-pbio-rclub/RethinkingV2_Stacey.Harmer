---
title: "Rethinking_week05_2019-05-03"
author: "Stacey Harmer"
date: "4/30/2019"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Video 4 

line = 1st order polynomial
parabola = 2nd order polynomial

almost always: center predictor, and then divide by SD to make Z scores.  value 1 means 1 SD from mean

(Read up more on Z scores)

splines
spline is a bar with metal weight attached to bend it (knots); make smooth curves. 
locally wiggly!  basis function is a local function. (basis means component, here)

can tune individual local functions. 

predictor variables are 'synthetic data' that defines range of curve.
actual predictor of interest is not in your model!!
B define ranges that each parameter is releveant to; that parement is w, or weight.  like slope.
B variables turn on parameters over differnt ranges of x axis 

Basis function B are pretty arbitrary, and not based on any observed data.  

degree 1 basis funciton = straight line

cubic splines - flexible but not crazy.  commonly used. 

spline slides - I don't understand the middle graph in figure 4.13.  

Julin comments:  two types of predcitions.  one is for model parameters (mean, SD, etc for the popultion)
BUT also of interest woudl be predictions about where new data might lie relative to pre-existing data.  This is PREDICTION INTERVAL.  

## Reading: Finish chapter 4  (I need to start on page 80)

```{r}
library(rethinking)
data(Howell1)

d <- Howell1
precis(d)
```
```{r}
d2 <- d[ d$age >= 18 ,]
plot(d2$height)
dens(d2$height)
```

```{r}
curve( dnorm(x, 178, 20), from = 100, to=250)
curve( dunif(x, 0, 50), -10, 60)

```
p 83
Now, simulate heights by sampling from prior
code 4.14
```{r}
sample_mu <- rnorm(1e4, 178, 20)
sample_sigma <- runif(1e4, 0, 50)
prior_h <- rnorm(1e4, sample_mu, sample_sigma)
dens(prior_h)
```
### section 4.3.3  grid approximation

```{r}
mu.list <- seq( from=140, to=160 , length.out=200 ) 
sigma.list <- seq( from=4 , to=9 , length.out=200 )
post <- expand.grid( mu=mu.list , sigma=sigma.list )

post$LL <- sapply( 1:nrow(post) , function(i) sum( dnorm(
  d2$height ,
  mean=post$mu[i] ,
  sd=post$sigma[i] ,
  log=TRUE ) ) )

post$prod <- post$LL + dnorm( post$mu , 178 , 20 , TRUE ) +
  dunif( post$sigma , 0 , 50 , TRUE )

post$prob <- exp( post$prod - max(post$prod) )

contour_xyz(post$mu, post$sigma, post$prob)

image_xyz(post$mu, post$sigma, post$prob)
```

OK, now sample from this posterior distribution. randomly sample row numbers in proportion to values in post$prob

```{r}
sample.rows <- sample( 1:nrow(post) , size=1e4 , replace=TRUE ,
  prob=post$prob )
?sample

sample.mu <- post$mu[ sample.rows ]
sample.sigma <- post$sigma[ sample.rows ]

plot( sample.mu , sample.sigma , cex=0.5 , pch=16 , col=col.alpha(rangi2,0.1) )

```
summarize samples (paramaters)
These (sample.mu) are marginal posterior densities; marginal here means averaging over other parameters

```{r}
dens(sample_mu)
dens(sample.mu)  # weird.  plotting artifact?

dens(sample_sigma)
dens(sample.sigma)

HPDI(sample.mu)
HPDI(sample.sigma)

```

### 4.3.5  Quap
p 88
quadratic approximation as way to make inferences about shape of posterior.  

```{r}
precis(d2)
```
code 4.27
```{r}
flist <- alist(
    height ~ dnorm( mu , sigma ) ,
    mu ~ dnorm( 178 , 20 ) ,
    sigma ~ dunif( 0 , 50 )
)

m4.1 <- quap(flist, data = d2)
precis(m4.1)

HPDI(sample.mu)
HPDI(sample.sigma)
```
list vs alist - list executes code, but alist does not.  
so for list of formulas, use alist!
p 89  (p 98 is end of previous assignment .. . )

```{r}
m4.2 <- quap(
    alist(
      height ~ dnorm( mu , sigma ) ,
      mu ~ dnorm( 178 , 0.1 ) ,
      sigma ~ dunif( 0 , 50 )
    ) , data=d2 )

precis( m4.2 )


```

multi-dimenstional Gaussian distribution
```{r}
vcov( m4.1)

diag( vcov( m4.1 ) )
cov2cor( vcov( m4.1 ) )

```

to sample from multi-dimensional posterior, need to sample vectors of vlues

```{r}
post <- extract.samples( m4.1, n = 1e4)
head(post)
precis(post)
```
### 4.4  - add predictor.  
how is an outcome (height) related to predictor (weight) ?

```{r}
plot( d2$height ~ d2$weight)

```
linear model

```{r}
set.seed(2971) 
N <- 100 # 100 lines
a <- rnorm( N , 178 , 20 )
b <- rnorm( N , 0 , 10 )
```
plot
```{r}
plot( NULL , xlim=range(d2$weight) , ylim=c(-100,400) , 
    xlab="weight" , ylab="height" )
abline( h=0 , lty=2 )
abline( h=272 , lty=1 , lwd=0.5 )
mtext( "b ~ dnorm(0,10)" )
xbar <- mean(d2$weight)
for ( i in 1:N ) curve( a[i] + b[i]*(x - xbar) ,
  from=min(d2$weight) , to=max(d2$weight) , add=TRUE ,
  col=col.alpha("black",0.2) )

```
make sure beta is always positive.  to do this, use log normal!!
thta is, claim log of beta has normal distribution

```{r}
b <- rlnorm(1e4, 0, 1)
dens(b, xlim=c(0,5), adj = 0.1)

plot( NULL , xlim=range(d2$weight) , ylim=c(-100,400) , 
    xlab="weight" , ylab="height" )
abline( h=0 , lty=2 )
abline( h=272 , lty=1 , lwd=0.5 )
mtext( "b ~ dnorm(0,10)" )
xbar <- mean(d2$weight)
for ( i in 1:N ) curve( a[i] + b[i]*(x - xbar) ,
  from=min(d2$weight) , to=max(d2$weight) , add=TRUE ,
  col=col.alpha("black",0.2) )


```

```{r}
set.seed(2971)
N <- 100 # 100 lines
a <- rnorm( N , 178 , 20 )
b <- rlnorm( N , 0 , 1 )
```

4.4.2   posterior

```{r}
# load data again, since it's a long way back
library(rethinking)
data(Howell1)
d <- Howell1
d2 <- d[ d$age >= 18 , ]

# define the average weight, x-bar
xbar <- mean(d2$weight)

# fit model
m4.3 <- quap(
  alist(
    height ~ dnorm( mu , sigma ) ,
    mu <- a + b*( weight - xbar ) ,
    a ~ dnorm( 178 , 20 ) ,
    b ~ dlnorm( 0 , 1 ) ,
    sigma ~ dunif( 0 , 50 )
  ) ,
  data=d2 )
precis(m4.3)

```

4.4.3

plotting!

```{r}
plot( height ~ weight , data=d2 , col=rangi2 ) 
post <- extract.samples( m4.3 )
a_map <- mean(post$a)
b_map <- mean(post$b)
curve( a_map + b_map*(x - xbar) , add=TRUE ) 


```

## why did this work?  where did x come from in above plot?  how did R know x was weight?

estimate model with only 10 samples from original dataset
```{r}
N <- 10
dN <- d2[ 1:N , ]

mN <- quap(
  alist(
    height ~ dnorm( mu , sigma ) ,
    mu <- a + b*( weight - mean(weight) ) ,
    a ~ dnorm( 178 , 20 ) ,
    b ~ dlnorm( 0 , 1 ) ,
    sigma ~ dunif( 0 , 50 )
  ) , data=dN )
```

now plot 20 lines from resulting model (sampling from posterior) 
```{r}
# extract 20 samples from the posterior
post <- extract.samples( mN , n=20 )

# display raw data and sample size
plot( dN$weight , dN$height ,
      xlim=range(d2$weight) , ylim=range(d2$height) ,
      col=rangi2 , xlab="weight" , ylab="height" )
mtext(concat("N = ",N))

# plot the lines, with transparency
for ( i in 1:20 )
  curve( post$a[i] + post$b[i]*(x-mean(dN$weight)) ,
       col=col.alpha("black",0.3) , add=TRUE )
```
Mess around with additional datapoints
```{r}
N <- 200
dN <- d2[ 1:N , ]

mN <- quap(
  alist(
    height ~ dnorm( mu , sigma ) ,
    mu <- a + b*( weight - mean(weight) ) ,
    a ~ dnorm( 178 , 20 ) ,
    b ~ dlnorm( 0 , 1 ) ,
    sigma ~ dunif( 0 , 50 )
  ) , data=dN )

# extract 20 samples from the posterior
post <- extract.samples( mN , n=20 )

# display raw data and sample size
plot( dN$weight , dN$height ,
      xlim=range(d2$weight) , ylim=range(d2$height) ,
      col=rangi2 , xlab="weight" , ylab="height" )
mtext(concat("N = ",N))

# plot the lines, with transparency
for ( i in 1:20 )
  curve( post$a[i] + post$b[i]*(x-mean(dN$weight)) ,
       col=col.alpha("black",0.3) , add=TRUE )

```

Say I want to display uncertainty in height around a single weight
```{r}
post <- extract.samples( m4.3 )
mu_at_50 <- post$a + post$b * ( 50 - xbar )

dens( mu_at_50 , col=rangi2 , lwd=2 , xlab="mu|weight=50" )
```
Note that adding Gaussian distributions always produces Gaussian distributions.

OK, repeat this for every weight value
link function makes this easy

```{r}
mu <- link( m4.3 ) 
str(mu)
# head(mu)
```

## link takes quap approximation, samples from posterior distribution, and computes mu for each case in the data and sample from the poterior distribution

HUH?  Well, returns matrix.  each row is sample from posterior (1k is default); each column represents a case in the data (original data).  
```{r}
mu[1:10, 1:10]

```
```{r}
head(d2)  # these are systematically smaller than in mu
```

## Why don't the values in 'mu' match the values in d2 better????

```{r}
# define sequence of weights to compute predictions for 
# these values will be on the horizontal axis
weight.seq <- seq( from=25 , to=70 , by=1 )
# use link to compute mu
# for each sample from posterior
# and for each weight in weight.seq
mu <- link( m4.3 , data=data.frame(weight=weight.seq) )
str(mu)
```
plot
```{r}
# use type="n" to hide raw data 4.55
plot( height ~ weight , d2 , type="n" )
# loop over samples and plot each mu value
for ( i in 1:100 )
points( weight.seq , mu[i,] , pch=16 , col=col.alpha(rangi2,0.1) )

```

summarize

```{r}
# summarize the distribution of mu
mu.mean <- apply( mu , 2 , mean )
mu.HPDI <- apply( mu , 2 , HPDI , prob=0.89 )

```
plot

```{r}
# plot raw data
# fading out points to make line and interval more visible
plot( height ~ weight , data=d2 , col=col.alpha(rangi2,0.5) )
# plot the MAP line, aka the mean mu for each weight
lines( weight.seq , mu.mean )
# plot a shaded region for 89% HPDI
shade( mu.HPDI , weight.seq )


```
p 107 - nice summary of process.  

#### 4.4.3.5  Prediction intervals
prediction intervals for actual heights, not just average height.  need to use sigma for this.

sim will do this, using mu and sigma from posteriod distribution

```{r}
sim.height <- sim(m4.3, data = list(weight = weight.seq))
str(sim.height)
height.PI <- apply( sim.height , 2 , PI , prob=0.89 )

```

plot it all
```{r}
# plot raw data 4
plot( height ~ weight , d2 , col=col.alpha(rangi2,0.5) )

# draw MAP line
lines( weight.seq , mu.mean )

# draw HPDI region for line
shade( mu.HPDI , weight.seq )

# draw PI region for simulated heights
shade( height.PI , weight.seq)

```

## 4.5   Curves!!

```{r}
library(rethinking) 
data(Howell1)
d <- Howell1
str(d)

plot(d$weight, d$height)
```
to fit polynomial, first standardize predictor (weight)

how to assign prior for beta-2? He asserts we don't want oto limit to a positive number

```{r}
d$weight_s <- ( d$weight - mean(d$weight) )/sd(d$weight)  # Z scores

d$weight_s2 <- d$weight_s^2

m4.5 <- quap(
  alist(
    height ~ dnorm( mu , sigma ) ,
    mu <- a + b1*weight_s + b2*weight_s2 ,
    a ~ dnorm( 178 , 20 ) ,
    b1 ~ dlnorm( 0 , 1 ) ,
    b2 ~ dnorm( 0 , 1 ) ,
    sigma ~ dunif( 0 , 50 )
    ) ,
  data=d )
```

```{r}
precis(m4.5)
```
Note that value of a no longer = mean height in the sample.

plot
```{r}
weight.seq <- seq( from=-2.2 , to=2 , length.out=30 ) 
pred_dat <- list( weight_s=weight.seq , weight_s2=weight.seq^2 )
mu <- link( m4.5 , data=pred_dat )
mu.mean <- apply( mu , 2 , mean )
mu.PI <- apply( mu , 2 , PI , prob=0.89 )
sim.height <- sim( m4.5 , data=pred_dat )
height.PI <- apply( sim.height , 2 , PI , prob=0.89 )
```
plot
```{r}
plot( height ~ weight_s , d , col=col.alpha(rangi2,0.5) ) 
lines( weight.seq , mu.mean )
shade( mu.PI , weight.seq )
shade( height.PI , weight.seq )
```
And now a cubic version!
```{r}
d$weight_s3 <- d$weight_s^3

m4.6 <- quap(
  alist(
    height ~ dnorm( mu , sigma ) ,
    mu <- a + b1*weight_s + b2*weight_s2 + b3*weight_s3 ,
    a ~ dnorm( 178 , 20 ) ,
    b1 ~ dlnorm( 0 , 1 ) ,
    b2 ~ dnorm( 0 , 10 ) ,
    b3 ~ dnorm( 0 , 10 ) ,
    sigma ~ dunif( 0 , 50 )
    ) ,
  data=d )
    
    
```

Bonus:  plot data on original scale, not z scores
```{r}
plot( height ~ weight_s , d , col=col.alpha(rangi2,0.5) , xaxt="n" )  # x axis not plotted
at <- c(-2,-1,0,1,2) 
labels <- at*sd(d$weight) + mean(d$weight)
axis( side=1 , at=at , labels=round(labels,1) )


```

### Splines!  

```{r}
library(rethinking) 
data(cherry_blossoms)
d <- cherry_blossoms
precis(d)
```

```{r}

plot(d$year, d$temp)

```
B-splines - invent new, synthetic predictor values. Each is applied within specific range, and is called a basis function.
The B variables (basis functions) tell you which 'knot' you are close to.

the parameter 'w' determines influence of each basis function.  these weight parameters were obtained by fitting the model to the data! These can be positive or negative. 

Note: instead of linear approximations for the B variables, we could use polynomials! 

First, make model with knots placed at quantiles of predictor variable
```{r}
d2 <- d[ complete.cases(d$temp) , ] # complete cases on temp 
num_knots <- 15
knot_list <- quantile( d2$year , probs=seq(0,1,length.out=num_knots) )
length(knot_list)

```

Now for splines, must choose polynomial degree.  Degree 1 was our first example; here, 2 basis functions combine at each point.  Let's try cubic spline (4 basis functions combine per point)

```{r}
library(splines)
B <- bs(d2$year,
        knots=knot_list[-c(1,num_knots)] ,
        degree=6 , intercept=TRUE )

B[1:10, ]
```

## WHY are there 17 columns here?  there were 15 knots (which start at beginning and end at end of data, I believe )

Now try this model

```{r}
m4.7 <- quap( 
  alist(
    T ~ dnorm( mu , sigma ) ,
    mu <- a + B %*% w ,
    a ~ dnorm(6,10),
    w ~ dnorm(0,1),
    sigma ~ dexp(1)
    ),
  data=list( T=d2$temp , B=B ) ,
  start=list( w=rep( 0 , ncol(B) ) ) )
```

```{r}
precis(m4.7, depth = 2)

```

OK, plot posterior predictions

```{r}
post <- extract.samples(m4.7)
w <- apply( post$w , 2 , mean )
plot( NULL , xlim=range(d2$year) , ylim=c(-2,2) ,
      xlab="year" , ylab="basis * weight" )
for ( i in 1:ncol(B) ) lines( d2$year , w[i]*B[,i] )



```
And now determine posterior intervals for mu

```{r}
mu <- link(m4.7)

mu_PI <- apply(mu, 2, PI, 0.97)
plot(d2$year, d2$temp, col = col.alpha(rangi2, 0.3), pch = 16)
shade(mu_PI, d2$year, col = col.alpha("black", 0.5))

```

### OK, as above, but increase # knots to 30

```{r}
d2 <- d[ complete.cases(d$temp) , ] # complete cases on temp 
num_knots <- 30
knot_list <- quantile( d2$year , probs=seq(0,1,length.out=num_knots) )
length(knot_list)

B <- bs(d2$year,
        knots=knot_list[-c(1,num_knots)] ,
        degree=3 , intercept=TRUE )

B[1:10, 1:17]

m4.8 <- quap( 
  alist(
    T ~ dnorm( mu , sigma ) ,
    mu <- a + B %*% w ,
    a ~ dnorm(6,10),
    w ~ dnorm(0,1),
    sigma ~ dexp(1)
    ),
  data=list( T=d2$temp , B=B ) ,
  start=list( w=rep( 0 , ncol(B) ) ) )

mu <- link(m4.8)

mu_PI <- apply(mu, 2, PI, 0.97)
plot(d2$year, d2$temp, col = col.alpha(rangi2, 0.3), pch = 16)
shade(mu_PI, d2$year, col = col.alpha("black", 0.5))

```
Indeed, it is wigglier now.


## Problems: 4M4, 4M5, 4M6, 4H1, 4H2

I think I'd better start with 4M1

### 4M1
For the model definition below, simulate observed heights from the prior (not the posterior).

```{r}
sample_mu <- rnorm(1e4, 0,10 )
sample_sigma <- runif(1e4, 0, 10)

prior_y <- rnorm(1e4, sample_mu, sample_sigma)

dens(prior_y)
```

### 4M2
Make the above a quap function.  What to do about data?
```{r}
#m4.m2 <- quap(
#  alist(
#    y ~ dnorm(mu, sigma),
#   mu ~ dnorm(0, 10),
#    sigma ~ dunif(0, 10)
#  )
#)

# can't actually run without data
```
### 4M3
Translate quap below into mathematical definitions

```{r}
flist <- alist(
    y ~ dnorm( mu , sigma ),
    mu <- a + b*x,
    a ~ dnorm( 0 , 50 ),
    b ~ dunif( 0 , 10 ),
    sigma ~ dunif( 0 , 50 )
)
```


$$ y_i \sim Normal (\mu_i, \sigma) \ mu_i = \alpha + \beta x_i \ \alpha \sim Normal(0, 50) \ \beta \sim Uniform(0, 10) \ \sigma \sim Uniform(0, 50) $$
y ~ normal(mu, sigma)
mu = a + beta * x
a ~ normal(0, 50)
b ~ uniform(0, 10)
sigma ~ uniform(0, 50)

### 4M4
A sample of students is measured for height each year for 3 years. After the third year, you want to fit a linear regression predicting height using year as a predictor. Write down the mathematical model definition for this regression, using any variable names and priors you choose. Be prepared to defend your choice of priors.

height ~ Normal(mu, sigma)
mu ~ a + beta*year
a ~ Normal(150, 20)       
*I'm assuming that students are about 12 years old; internet search shows mean height in US about 150 cm. SD of 20 seems pretty generous*
beta ~ Log-Normal(0, 1)  
*I'm assuming there is no shrinkage over time, and that most students don't grow much over three years*
sigma ~ dunif(0, 50)
*This seems pretty broad to me; maybe should be more restricted?*

### 4M5
Now suppose I tell you that the average height in the first year was 120 cm and that every student got taller each year. Does this information lead you to change your choice of priors? How?

Yes, now I would change my a prior to be:
a ~ Normal(120, 20)     
And maybe I'd change my beta as well, since beta should always be > 0
beta ~ Log-Normal(0.1, 1) 

How different is that prior?

```{r}
dens(rlnorm( 1e4 , 0 , 1 )) 
```
```{r}
dens(rlnorm( 1e4 , 0.1 , 1 ))
```
Subtle difference by eye.

### 4M6
Now suppose I tell you that the variance among heights for students of the same age is never more than 64cm. How does this lead you to revise your priors?

I'd want to limit my sigma value so absolute max is 64
So now I'd have:

height ~ Normal(mu, sigma)
mu ~ a + beta*year
a ~ Normal(120, 20)       
*mean value provided*
beta ~ Log-Normal(0.1, 1)  
*I'm assuming there is no shrinkage over time, and that all students grow at least some*
sigma ~ dunif(0, 64)
*using suggested max variance*

#### oops - should have chosen sigma ~ dunif(0, 8) because square of value.  

### 4H1
The weights listed below were recorded in the !Kung census, but heights were not recorded for these individuals. Provide predicted heights and 89% intervals (either HPDI or PI) for each of these individuals.

I think that I need to regenerate models from the chapter, and then retrieve mu and HPDI for each of the 5 given weights.
Given these weights, I will stick to the over 18 subset of the data

```{r}
library(rethinking) 
data(Howell1)
d <- Howell1
d.4h1 <- d[ d$age >= 18 , ]
precis(d.4h1)
```
In my case, I'll use the population mean as the prior; why not?  mean height is 155; 

my linear model will be mu = a + beta*weight.  I'll use quap  (see p 98)

```{r}
# avg weight defined
xbar <- mean(d.4h1$weight)  # about 45 kg

m.4h1 <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b*(weight - xbar),
    a ~ dnorm(155, 20),
    b ~ dlnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = d.4h1
)

precis(m.4h1)
```
Now I think that I want to extract samples from the posterior.
```{r}
post.4h1 <- extract.samples( m.4h1 , n=1e4 )

```
And now I want to determine what intervals are for given weights

First one is 46.95; test it

#### here, I just recaptiulated the link funciton.  
```{r}
mu.1 <- post.4h1$a + post.4h1$b * ( 46.95 - xbar )
length(mu.1) # 1e4, vector I believe
dens( mu.1 , col=rangi2 , lwd=2 , xlab="mu|weight=1st" )
```
and
```{r}
chainmode( mu.1 , adj=0.01 ) # 156.35 is the 'most expected' height

HPDI( mu.1 , prob=0.89 )
```
Can I do this systematically for all 5 values?
I am sure there is clever loop way to do this, but how to set it up?

Or why not try sapply instead???

#### I should have used sim function for this !!!
```{r}
cases <- c(46.95, 43.72, 64.78, 32.59, 54.63)

samples.all <- sapply( cases , function(c) post.4h1$a + post.4h1$b * ( c - xbar ) )  # I think that looks right

# now determine 'most expected' from each column; 

apply(samples.all, 2, function(x) chainmode(x, adj=0.01 ))

# and now determine HPDI

apply(samples.all, 2, function(x) HPDI(x, prob=0.89 ))

```

Not pretty, but seemed to work

### 4H2
Select out all the rows in the Howell1 data with ages below 18 years of age. If you do it right, you should end up with a new data frame with 192 rows in it.

```{r}
data(Howell1)
d <- Howell1
d.4h2 <- d[ d$age < 18 , ]
precis(d.4h2) # mean height is 108; SD is 26
```
(a) Fit a linear regression to these data, using quap. Present and interpret the estimates. For every 10 units of increase in weight, how much taller does the model predict a child gets?

```{r}
# avg weight defined
xbar.h2 <- mean(d.4h2$weight)  # about 18.4 kg

m.4h2 <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b*(weight - xbar.h2),
    a ~ dnorm(100, 40),   #kind of wide prior
    b ~ dlnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = d.4h2
)

precis(m.4h2)

```
The population mean is estimated as 108.32 (same as in actual data), with 89% interval between 107.4 and 109.3
beta is estimated at 2.72.  So for every 10 units more weight, we'd expect child to be 10*2.72 = 27.2 units taller.

(b) Plot the raw data, with height on the vertical axis and weight on the horizontal axis. Superimpose the MAP regression line and 89% HPDI for the mean. Also superimpose the 89% HPDI for predicted heights.

#### should have used sim fnction!! and link function too.

see p 109
```{r}
# first, extract samples from posterior
post.4h2 <- extract.samples(m.4h2)
mu.link <- function(weight) post.4h2$a + post.4h2$b*( weight - xbar.h2 )

# define range of weights for plotting
weight.seq <- seq( from=0 , to=50 , by=1 )

mu.4h2 <- sapply( weight.seq , mu.link )
mu.mean.4h2 <- apply( mu.4h2 , 2 , mean )
mu.HPDI.4h2 <- apply( mu.4h2 , 2 , HPDI , prob=0.89 )


# and obtain simulated/predicted heights
sim.height.4h2 <- sim( m.4h2 , data=list(weight=weight.seq) )
str(sim.height.4h2)

# summarize simulated heights
height.HPDI.4h2 <- apply( sim.height.4h2 , 2 , HPDI , prob=0.89 )
```

OK, do I have all I need now?

```{r}
# plot raw data 
plot( height ~ weight , d.4h2 , col=col.alpha(rangi2,0.5) )

# draw MAP line
lines( weight.seq , mu.mean.4h2 )

# draw HPDI region for line
shade( mu.HPDI.4h2 , weight.seq )

# draw HPDI region for simulated heights
shade( height.HPDI.4h2 , weight.seq )

```
(c) What aspects of the model fit concern you? Describe the kinds of assumptions you would change, if any, to improve the model. You donâ€™t have to write any new code. Just explain what the model appears to be doing a bad job of, and what you hypothesize would be a better model.

The data don't fit the line well; the relationship between weight and height clearly isn't linear over the entire range.  (Seems at low weights, rapid increases in height but not weight; at high weights, the opposite. )  So the model is doing a very poor job of predicting relationships at both the high and the low ends of the data range.

For a better model, I might try using B-spline, or perhaps quadratic regression.  


