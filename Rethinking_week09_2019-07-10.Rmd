---
title: "R Notebook"
output: 
  html_document: 
    keep_md: yes
---
R club week 9 (7/10/2019)

paths: information can walk against the arrows, but causation only flows with the arrows.
implied conditional independence.  
(I think one trick is to see if collider is present)
directed acyclic gaffes (!)  put error into model; residual confounding.

_Video: Ulysses_ Compass https://youtu.be/0Jc6Kgw5qc0 (you may have already watched the first half of this)
starts at minute 43 ish

overfitting  - learning too  much from the data.  fit well, but predict worse.
(classic is polynomial regression - wiggles aroudn like carzy when drop out any one point)

underfitting  - learning too little from the data.  fit and predict poorly.

fit model, dropping out one data point at a time.  insenstiive to exact sample - maybe too little fitting.

(* adding parameters improves fit to sample, although not true of multilevel models & other types)

R-squared - common measure of model fit. = 1 - var(resid)/var(outcome)

Regularization of priors.  (penalized likelihood is same thing)

WAIC - widely applicable information criteria (has replaced AIC).  

_Book: 7.1 and 7.2_

regularizing prior = penalized likelihood.  prior limits how much model relies on the data. 

Information criteria - estimate the predictive accuracy of the model. 

information theory

**7.1 parameters**

```{r}
sppnames <- c( "afarensis","africanus","habilis","boisei",
               "rudolfensis","ergaster","sapiens")
brainvolcc <- c( 438 , 452 , 612, 521, 752, 871, 1350 )
masskg <- c( 37.0 , 35.5 , 34.5 , 41.5 , 55.5 , 61.0 , 53.5 )
d <- data.frame( species=sppnames , brain=brainvolcc , mass=masskg )
```
now rescale varialbes

```{r}
d$mass_std <- (d$mass - mean(d$mass))/sd(d$mass) #mean zero, SD =1
d$brain_std <- d$brain / max(d$brain)
summary(d)
```

code 7.3
```{r}
library(rethinking)
m7.1 <- quap(
  alist(
    brain_std ~ dnorm( mu , exp(log_sigma) ),
    mu <- a + b*mass_std,
    a ~ dnorm( 0.5 , 1 ),
    b ~ dnorm( 0 , 10 ),
    log_sigma ~ dnorm( 0 , 1 )
), data=d )
summary(d)
```
***why the exp(log)??***

R-sq tells us 'proportion of variance "explained" by the model'.  
compute posterior predictive distribution for each observation.  then get residuals, and variances.

code 7.4
```{r}
set.seed(12)
s <- sim( m7.1 )  # this computes posterior predictive distribution for each observation.  
summary(s)
# see figure 3.6 on page 66 for explanation
r <- apply(s,2,mean) - d$brain_std  # r is residual
resid_var <- var2(r) # var2 is actual empirical variance
outcome_var <- var2( d$brain_std )
1 - resid_var/outcome_var

```
write function
code 7.5
```{r}
R2_is_bad <- function( quap_fit ) {
  s <- sim( quap_fit , refresh=0 )
  r <- apply(s,2,mean) - d$brain_std
  1 - var2(r)/var2(d$brain_std)
}
```



```{r}
m7.2 <- quap( 
  alist(
    brain_std ~ dnorm( mu , exp(log_sigma) ),
    mu <- a + b[1]*mass_std + b[2]*mass_std^2,
    a ~ dnorm( 0.5 , 1 ),
    b ~ dnorm( 0 , 10 ),
    log_sigma ~ dnorm( 0 , 1 )
    ), data=d , start=list(b=rep(0,2)) )

```


```{r}
m7.3 <- quap( 
  alist(
    brain_std ~ dnorm( mu , exp(log_sigma) ),
    mu <- a + b[1]*mass_std + b[2]*mass_std^2 + 
      b[3]*mass_std^3,
    a ~ dnorm( 0.5 , 1 ),
    b ~ dnorm( 0 , 10 ),
    log_sigma ~ dnorm( 0 , 1 )
    ), data=d , start=list(b=rep(0,3)) )

m7.4 <- quap( 
  alist(
    brain_std ~ dnorm( mu , exp(log_sigma) ),
    mu <- a + b[1]*mass_std + b[2]*mass_std^2 + 
      b[3]*mass_std^3 + b[4]*mass_std^4,
    a ~ dnorm( 0.5 , 1 ),
    b ~ dnorm( 0 , 10 ),
    log_sigma ~ dnorm( 0 , 1 )
    ), data=d , start=list(b=rep(0,4)) )

m7.5 <- quap( 
  alist(
    brain_std ~ dnorm( mu , exp(log_sigma) ),
    mu <- a + b[1]*mass_std + b[2]*mass_std^2 + 
      b[3]*mass_std^3 + b[4]*mass_std^4 +
      b[5]*mass_std^5,
    a ~ dnorm( 0.5 , 1 ),
    b ~ dnorm( 0 , 10 ),
    log_sigma ~ dnorm( 0 , 1 )
    ), data=d , start=list(b=rep(0,5)) )


```
last model: replace SD with constant

code 7.8
```{r}
m7.6 <- quap(
  alist(
    brain_std ~ dnorm( mu , 0.001 ),
    mu <- a + b[1]*mass_std + b[2]*mass_std^2 +
      b[3]*mass_std^3 + b[4]*mass_std^4 +
      b[5]*mass_std^5 + b[6]*mass_std^6,
    a ~ dnorm( 0.5 , 1 ),
    b ~ dnorm( 0 , 10 )
    ), data=d , start=list(b=rep(0,6)) )
```

plot
```{r}
post <- extract.samples(m7.1)
mass_seq <- seq( from=min(d$mass_std) , to=max(d$mass_std) , length.out=100 )
l <- link( m7.1 , data=list( mass_std=mass_seq ) )
mu <- apply( l , 2 , mean )
ci <- apply( l , 2 , PI )
plot( brain_std ~ mass_std , data=d )
lines( mass_seq , mu )
shade( ci , mass_seq )
```
code 7.10
```{r}
m7.1_OLS <- lm( brain_std ~ mass_std , data=d )
post <- extract.samples( m7.1_OLS )
```

code 7.11 - no predictor variables

```{r}
m7.7 <- quap( 
  alist(
    brain_std ~ dnorm( mu , exp(log_sigma) ),
    mu <- a,
    a ~ dnorm( 0.5 , 1 ),
    log_sigma ~ dnorm( 0 , 1 )
    ), data=d )

```

helpful trick - see function brain_loo_plot  
***couldn't find this function***
```{r}
??brain_loo_plot
```


**7.2 entropy, accuracy**
what is out of sample deviance?  deviance of model from future observations

7.2.1  
Information Entropy
p 218. "uncertainty in a probability distribution ishte average log-probability of an event"

code 7.13
```{r}
p <- c(0.3, 0.7)
-sum(p*log(p))  # this is the entropy value.  this quantifies our uncertainty
# 0.61

p <- c(0.5, 0.5)
-sum(p*log(p))
# 0.69

p <- c(0.95, 0.05)
-sum(p*log(p))
# 0.20
```
maxent = maximum entropy = family of techniques to find probabliliyt distributions more consistent with our state of knowledge.  

7.2.3  
entropy to accuracy

Divergence = additonal uncertainty introduced by using probabilites from one disribution to describe another.
It is the average differnce in log prob between target (p) and model (q).  the differnece between entropy of hte target distribution p, and the model distribution q.

can use this to contrast different approximations to p.  want to minimize divergence.  model with less divergence is more accurate.

K-L divergence (above) is distance of model from target
can estimate this divergence using deviance

***need to compare the average log-probabilities of the two models in question.***  

code 7.14
```{r}
set.seed(1)
lppd( m7.1, n=1e4)

```
lppd = log pointwise predictive density.  
sum them up to get total log-prob score for model and data

code 7.15
```{r}
set.seed(1) 
logprob <- sim( m7.1 , ll=TRUE , n=1e4 )
n <- ncol(logprob)
ns <- nrow(logprob)
f <- function( i ) log_sum_exp( logprob[,i] ) - log(ns)
( lppd <- sapply( 1:n , f ) )
log(1) # 0
log(0.001) # -6.9
exp(log(0.001)) # 0.001

```


7.2.5  
scoring the right data

```{r}
set.seed(1)
sapply( list(m7.1,m7.2,m7.3,m7.4,m7.5,m7.6) , function(m) sum(lppd(m)) )
```
p224 
**confusing**
the 'true' model has non-zero coeff for only first 2 predictors, so the true model has 3 parameters ????

_Problems: 7E1 - 7E4_ 
(actually listed as 6E1 - 6E4 in the book but they are at the end of chapter 7 on PAGE 234/250)

**7E1**
the three motivating criteria that define information entropy.  see p 202
1. we want the relationship between probability and uncertainty to be continuous.  this way, small changes in probability don't cause big changes in uncertainty.
2. uncertainty increases with the number of different possible outcomes.
3. uncertainty measurements can be added together so that subsets of conditions always add up so that total uncertainty is constant

**7E2**
What is the entropy of a coin that comes up heads 70% of the time?
on page 203, entropy is defined with these probabilites
```{r}
p <- c(0.7, 0.3)
-sum(p*log(p))
```

**7E3**
As above, but with 4 sides on a dice.
```{r}
p <- c(0.2, 0.25, 0.25, 0.3)
-sum(p*log(p))
```
**7E4**
As above, but with 4 sides on a dice, but one of which never comes up.
on page 203/219, he says just ignore this possible event (as it isn't really possible)

```{r}
p <- c(0.333, 0.333, 0.333)
-sum(p*log(p))

```

next time: watch next video, and then 7.2 and 7.3 



